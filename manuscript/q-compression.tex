\documentclass{bioinfo}

%\usepackage{doi}
\usepackage{url}

\copyrightyear{2015}
\pubyear{2015}

\begin{document}
\firstpage{1}

\title[lossy-compression]{Lossy compression of DNA sequencing quality data}
\author[El Hadidi \textit{et~al.}]{Mohamed El Hadidi\,$^{1}$\footnote{authors contributed equally}, Christopher M. Hill\,$^{2}$\footnotemark[1], Andr\'{a}s Szolek\,$^{3}$\footnotemark[1], and Michael P. Cummings\,$^4$\footnote{to whom correspondence should be addressed}}
\address{$^{1}$Department of Algorithms in Bioinformatics,  Center for Bioinformatics, University of T\"{u}bingen, Sand 14, 72076 T\"{u}bingen, Germany \\
$^{2}$Department of Computer Science, University of Maryland, College Park,  Maryland, 20742 USA\\
$^{3}$Department of Applied Bioinformatics, Center for Bioinformatics, Quantitative Biology Center, and Department of Computer Science, University of T\"{u}bingen, Sand 14, 72076 T\"{u}bingen, Germany\\
$^{4}$Center for Bioinformatics and Computational Biology, University of Maryland, College Park, 20742 USA}
%$\dagger$authors contributed equally
\history{Received on XXXXX; revised on XXXXX; accepted on XXXX}
% Hack to display authors contribution.
\editor{Associate Editor: XXXXXXX}

\maketitle

\begin{abstract}

\section{Motivation:}

The \textsc{fastq} file format has become the \emph{de facto} standard
for storing next-generation sequencing data, containing nucleotide
information along with a quantitative measure of the reliability of
individual base calls. As the cost of sequencing continues to
decrease, the rate of sequencing data production is increasing,
requiring efficient ways of storing and transferring this vast amount
of data. Most methods on sequencing data compression focus on
compressing nucleotide information without any loss of information.
Quality data, however, have different properties than nucleotide data,
and methods compressing nucleotide sequences efficiently do not
perform as well on quality sequences. Furthermore, while lossless
representation is necessary for nucleotide sequences, it is not an
essential requirement for quality values.

Existing methods for compressing quality sequences mostly focused on
minimizing the loss of information with less emphasis on effects on
subsequent analyses. In this paper, we evaluate several different
compression methods for quality values that compromise accuracy for
better storage efficiency, and their resulting impact on common
bioinformatic analyses using sequence read data.

\section{Results:}
Lossy compression of quality information can greatly decrease storage
and memory requirements with little discernible effects on subsequent
analysis results. The four compression strategies in this study were
able to produce similar results to those obtained with uncompressed
quality sequences in terms of quality control, genome assembly, and
alignment of short read to a reference sequence.

\section{Contact:} \href{mike@umiacs.umd.edu}{mike@umiacs.umd.edu}
\end{abstract}

\section{Introduction}

Read data from high-throughput sequencing constitutes the largest
category of data in genomics research because of great redundancy,
inclusion of quality values, and read-level naming and
metadata. Because of this abundance effective compression of read data
has the potential for substantial improvement in data storage and
transfer efficiency.

Quality values comprise a standard component of \textsc{fastq}
files~\citep{Cock:2010ve}, a very common format for sequence read
data. At the level of sequence read the probability of error for each
base-call is typically represented by \textsc{phred} quality value,
which is defined as $Q =
-10\,log_{10}P$~\citep{Ewing:1998ly}. Depending on the sequencing
technology these quality values can range from 0 to 93, and are
represented with the \textsc{ascii} characters 33 to 126 (with some
offset). There is a single quality value per base-call for Illumina
sequence reads.

Quality values can be used throughout bioinformatics pipelines. Among
the most fundamental uses of sequence quality values is as part of the
quality assessment and quality control (\textsc{qa/qc}) processes
prior to subsequent analysis steps. Quality control based on quality
values generally includes two operations: \textit{i}.~filtering, the
elimination of reads that on the whole do not meet arbitrary quality
standards, which reduces the total number of reads; and
\textit{ii}.~trimming of low quality base-calls from reads, which
reduces the number total number of bases. Quality values can be used
by genome assembly software to produce better
assemblies~\cite[e.g.,][]{Bryant:2009uq,Gnerre:2011kx}. Short-read
alignment software, such as Bowtie2~\citep{Langmead:2012rw}, use
quality values to weight mismatches between read and reference
sequences.  Software for detecting single nucleotide polymorphisms
(\textsc{snp}s) can use quality values~\cite[e.g.,][]{McKenna:2010bh},
and identified \textsc{snp}s with high-quality calls are deemed more
reliable than those with low-quality calls, particularly in low
coverage regions.

Previous literature on sequence data compression has largely focused
on lossless compression of base calls~\cite[reviewed
  in][]{Deorowicz:2013hq,Giancarlo:2014rw,Giancarlo:2009fk,
  Nalbantoglu:2010uq,Zhu:2013qr}, although some recent work has
addressed compression of quality
values~\cite[e.g.,][]{asnani2012lossy,Canovas:2014fr,Hach:2012ys,
  janin2013adaptive,Kozanitis:2011kl,Ochoa:2013rt,Tembe:2010ys,
  Wan:2012kq,DBLP:conf/recomb/YuYB14,zhou2014compression}. Among the
several challenges for compression of read data is dealing with
different error profiles resulting from differences in underlying
chemistries, signal detection and processing mechanisms, inherent
biases, and other idiosyncratic properties of distinct high-throughput
sequencing technologies. Sequence reads generated by instruments such
as an Illumina HiSeq, the focus of this research, are characterized by
having relatively few insertion and deletion errors, but substitution
(miscall) errors are much more frequent and have context-specific
patterns. These errors are non-uniformly distributed over the read
length (e.g., error rates increase up to $\sim$16$\times$ at the
3$^{\prime}$ end, and 32.8 -- 67.9\% of reads have low quality tails
at the 3$^{\prime}$ end~\citep{Minoche:2011km}).

Although we recognize the need for lossless compression for some
purposes and contexts (e.g., archiving, provenance), our perspective
is largely pragmatic with a focus on the use of quality values in
subsequent analyses. From this perspective some loss of information is
deemed acceptable if the inferences from analyses are relatively
unaffected. Here we describe our research investigating lossy
compression of sequence read quality values, specifically those
associated with Illumina instruments, with the objective to provide
some perspective on several strategies rather than to develop a robust
high-quality software for use. Recognizing these properties of
Illumina sequence reads motivates our exploration of three general
classes of lossy compression methods -- binning, modeling, and
profiling -- and consider an exemplar of each class. In addition to
quantifying compression efficiency, we also assess the effects of
quality value information loss resulting from compression on
subsequent genomic analyses including read preprocessing (filtering
and trimming), genome assembly, and read mapping.

\begin{methods}
\section{Methods}

\subsection{Compression strategies}

\subsubsection{Binning}

Quality values can be binned, and the minimum number of bins that
allows for a any distinction among quality values is two, i.e., two
categories ``good'' and ``bad'' quality. We implement 2-bin encoding
by setting a quality value threshold empirically determined by the
distribution of quality values across reads. Base-calls are marked
``bad'' if their quality value falls below the first quartile minus
1.5 $\times$ the interquartile range (IQR), which is the difference
between the first and third quartile. 1.5 $\times$ IQR is the value
used by Tukey's box plot~\citep{mcgill1978variations}. The main
benefit of this approach is that it is completely data-dependent, and
no assumptions regarding the distribution of the quality values need
to be made.
 
With 2-bin encoding binary encoding is possible, allowing us to use a
single bit to represent the quality of a base instead of the standard
8 bits used to store quality values in \textsc{ascii}. An additional
benefit of 2-bin encoding is the potential for increased adjacency of
identical values and repeating patterns, properties that may increase
effectiveness of subsequent compression using established
algorithms~\cite[e.g.,][]{HUFFMAN:1952nr,Ziv77auniversal,
  DBLP:journals/tit/ZivL78}.

The economic costs of memory use for binning, in general terms,
include no fixed costs, and marginal costs that are a function of the
number of base-call quality values times the cost of the encoding.

\subsubsection{Modeling}

If quality values are modeled compression is conceivably possible by
replacing the original quality values by a representation of the
model. For example, quality values can be conceptualized as bivariate
data with the ordered nucleotides (1 to read length) representing the
abscissa, and quality values representing the ordinate. In this
research we model read quality values as polynomial functions obtained
with least-squares fitting, as one approach to compression read
quality values by modeling.

Despite the fact that polynomial functions have significantly lower
number of parameters (i.e. one to six coefficients) than a read-length
string of raw quality values, the necessity of using floating point
numbers to store coefficients greatly limits the compression potential
of the method. In order to get meaningful compression on
single-precision four-byte floating point numbers, one would have to
relax on the least-squares approximation constraint to obtain
compressible values on the byte level which is outside the scope of
this study.

The economic costs of memory use for model-based compression, in
general terms, include no fixed costs, and marginal costs that are a
function of the number of reads times the cost representing the model
parameters.

\subsubsection{Profiling}

\begin{figure}[!tpb]%figure2
\centerline{\includegraphics[width=3.35in]{profiles_128.png}}
\caption{Quality profiles obtained by $k$-means clustering on the
  fragment library from \textit{Rhodobacter sphaeroides} 2.4.1 data
  set using $k$ = 128, with each row corresponding to a quality
  profile. Dark to light colors represent low to high quality
  values. It is readily visible that the two most distinctive features
  of quality profiles is their drop-off position and average overall
  quality. One can also see sporadic low-position values in a handful
  of profiles, likely capturing intermittent problems in the
  sequencing process affecting thousands of reads at a
  time.}\label{fig:profiles_128}
\end{figure}

\begin{figure*}[!tb]%figure2
%\centerline{\includegraphics[width=3.65in]{mse_frag.png}}
\centerline{\includegraphics[width=7in]{compression_results.pdf}}
\caption{Mean squared error versus bits/base-call for different
  compression methods applied to the \textit{Rhodobacter sphaeroides}
  2.4.1, and \textit{Homo sapiens} chromosome 14 fragment libraries,
  and \textit{Escherichia coli} str. K-12 MG1655, and \textit{Mus
    musculus} data sets. 2B --- 2-bin encoding; P$n$ 
  --- profiling with $n$ profiles; R$n$ --- modeling with polynomial regression
  models of degree $n$; Q$n$ --- QualComp with rate parameter 
  of $n$. Dashed line corresponds to lossless Bzip2
  compression of quality values, and dotted line corresponds to
  original uncompressed data.}
\label{fig:mse_vs_bpbp}
\end{figure*}

As large sets of quality strings show similar trends of quality over
their sequence, it makes sense to identify such common patterns in the
data and use them as reference profiles to approximate individual
quality sequences. Such patterns can be readily determined by
clustering data points (i.e. quality vectors) and using the resulting
cluster centers as representative profiles.

$k$-means clustering is a vector quantization method, partitioning a
set of samples into $k$ sets that minimize within-cluster sum of
squares~\citep{macqueen1967some}. Using a random subset of read
quality values, a compression method can use the computed cluster
centers as read quality profiles. As the problem is NP-hard, we use a
heuristic iterative refinement approach quickly converging to a
locally optimal minimum provided by the Python package scikit-learn
0.15~\citep{pedregosa2011scikit}.

First, the method samples an adjustable number of quality sequences
randomly from the file to be used as a training set. Quality sequences
are represented by vectors containing their \textsc{phred} scores
corresponding to each position along the read. Subsequently, $k$-means
clustering is performed on the training set until convergence. The
obtained cluster centers will be the quality profile prototypes for
the dataset.

Once the $k$ quality profiles are determined, all quality sequences
are passed through the trained $k$-means predictor, with the nearest
quality profile in Euclidean space being assigned to every quality
sequence as their compressed representation.

The compressed quality file therefore consists of an index enumerating
the $k$ quality profiles, and a binary part containing the assigned
quality profile index for each read.

Although this approach is not able to capture randomly occurring
outlier quality values, it ensures that the overall trends in quality
sequences are retained. Quality profiles capture different overall
qualities and different drop-off positions and gradients. An example
of 128 quality profiles are shown on Figure \ref{fig:profiles_128}.

The economic costs of memory use for profile-based compression, in
general terms, include fixed costs associated with representing the
profiles, which is a function of the number of profiles times the cost
of encoding them, and these fixed costs are amortized over the entire
set of reads to which they apply. Additionally there are marginal
costs that are a function of the number of reads encoded.

\subsection{Data sets}

We used several Illumina sequence read data sets in this research,
which are taken from data from the \textsc{gage} (Genome Assembly
Gold-Standard Evaluations)~\citep{Salzberg:2012rc} except as
noted. These data sets are as follows.

\textit{Rhodobacter sphaeroides} 2.4.1, which are generated from a
fragment library (insert size of 180 nt; 2,050,868 paired-end reads)
and short-jump library (insert size of 3,500 nt; 2,050,868 reads). The
corresponding reference sequence was obtained from the NCBI RefSeq
database (NC\_007488.1, NC\_007489.1, NC\_007490.1, NC\_007493.1,
NC\_007494.1, NC\_009007.1, NC\_009008.1).

\textit{Stapylococcus aureus} USA300, which are generated from a
fragment library (insert size of 180 nt; 1,294,104 paired-end reads)
and short-jump library (insert size of 3,500 nt; 3,494,070 reads).
The corresponding reference sequence was obtained from the NCBI RefSeq
database (NC\_010063.1, NC\_010079.1, NC\_012417.1).

\textit{Homo sapiens} chromosome 14 data, which are generated from a
fragment library (insert size of 155 nt; 36,504,800 paired-end reads)
and short-jump library (insert sizes ranging from 2283-2803 nt;
22,669,408 reads).  The corresponding reference sequence was obtained
from the NCBI RefSeq database (NC\_000014.8).

\textit{Escherichia coli} str. K-12 MG1655 MiSeq data was downloaded
from \url{http://www.illumina.com/systems/miseq/scientific_data.html},
which are generated from a fragment library (insert size of 180bp nt;
1,145,8940 paired-end reads).  The corresponding reference sequence
was obtained from the NCBI RefSeq database (NC\_000913.2).

\textit{Mus musculus} data was downloaded from
\url{http://trace.ddbj.nig.ac.jp/DRASearch/run?acc=SRR032209}, which
consisted of 18,828,274 reads of length 36.

\subsection{Performance evaluation}

As a measure of compression effectiveness we use bits/base-call, and
define it as the size of the compressed representation of quality
values (in bits) divided by the number of quality values represented.
As a measure of information loss we use mean squared error
(\textsc{mse}) as a loss function, and define it as
$\frac{1}{n}\sum_{i=1}^{n}{(Q_i'-Q_i)^2}$, where $n$ is the number of
sequences, $Q_i'$ is the compressed/decompressed quality value, and
$Q_i$ is the original quality value associated with sequence position
$i$.

We evaluate effects of information loss from quality value compression
on quality control steps of read filtering and trimming, which were
performed using Sickle~\citep{sickle}, quantify read characteristics
with PrinSeq~\citep{Schmieder:2011gd}, and make comparison to
uncompressed data.

We evaluate effects of information loss from quality value compression
on \emph{de novo} genome assembly performance using contiguity
statistics, log average read probability
(\textsc{lap})~\citep{Ghodsi:2013hb}, and a collection of
reference-based metrics. The contiguity statistics include number of
scaffolds and NG50, which is defined as the weighted median contig
size (the length of largest contig $c$ such that the total size of the
contigs larger than $c$â exceeds half of the known genome size). The
\textsc{lap} score can be viewed as a log likelihood score, where a
value closer to 0 is better. We use a reference-based evaluation
script provided by \textsc{gage} that counts single nucleotide
polymorphisms (\textsc{snp}s), relocations, translations, and
inversions. The reference-based metrics are normalized by the length
of the assembly to aid in comparison. For the genome assembly we used
software that makes use quality values in the assembly process:
\textsc{allpaths-lg}~\citep{Gnerre:2011kx} version r50191 with default
settings and 32 threads.

\end{methods}

\section{Results}

\subsection{Compression effectiveness versus information loss}

We compare the \textsc{mse} versus bits/base-call of the
\textit{Rhodobacter sphaeroides} 2.4.1, \textit{Homo sapiens}
chromosome 14, \textit{Escherichia coli} str. K-12 MG1655, and
\textit{Mus musculus} data sets (Figure \ref{fig:mse_vs_bpbp}). We
only include the fragment libraries for the \textit{Rhodobacter
  sphaeroides} 2.4.1, and \textit{Homo sapiens} chromosome 14 data
sets, but the additional short-jump library results are available in
the supplementary (TODO:TABLE). Storing the uncompressed quality
values requires 1 byte per base-call because they are stored in
\textsc{ascii} format and is denoted by the dotted black line in the
figure.  The lossless compression of each data set using BZip2 ranges
from 2.19 - 3.10 bits/base-call and is denoted by the colored dotted
lines.  The compression methods tend to cluster together across the
different data sets.  Across all data sets, the 0-order polynomial
regression, profile encodings, and QualComp have the lowest
bits/base-call.

QualComp with the rate parameter set to 100 bits/read has the lowest
\textsc{mse}, but requires 10-15x more storage than the profile
encoding methods for only a 2-3x reduction in \textsc{mse}.  When
QualComp's rate parameter is set to match the profile encoding
methods, QualComp performs slightly worse in terms of \textsc{mse}.
In the \textit{Rhodobacter sphaeroides} 2.4.1 fragment library,
QualComp with rate 10 bits/read (0.099 bits/bp) has a \textsc{mse} of
17.29.  Using 256-profile encoding requires less storage (0.079
bits/bp) and has a lower \textsc{mse} (11.85).

As the order of the polynomial increases, the bits/base-call increase
and the \textsc{mse} decreases at an exponential rate.  The 7th-order
polynomial regression has the highest bits/base-call and in the
\textit{Mus musculus} data set, and requires more storage than the
\textsc{ascii} original quality values.  A 7th-degree polynomial
requires storing eight floating point values, resulting in 32 bytes
per quality sequence.  The read length of the \textit{Mus musculus}
data set is only 26, so the 7th-degree polynomial regression is storing
six more bytes than necessary for lossless encoding the quality data.

\subsection{Effects on sequence read preprocessing}

Table \ref{tab:quality_control} shows the proportional numbers with
respect to those of the uncompressed \textsc{fastq} files. In the
fragment sample, we retain on average 95.4\% and 94.24\% of bases and
reads, respectively. In the short-jump sample, we retain 90.4\% and
90.98\% of bases and reads, respectively. In some cases, the
proportion of reads and/or bases compared to the original is more than
100\%; this means that the number of the reads and bases in the
compressed samples are more because altering quality values may
prevent the read from being filtered and/or trimmed. This is clear in
the good/ bad compression approach of the short-jump sample, where the
proportion of bases and reads are 113.09\% and 103.70\% respectively.

\subsection{Effects on genome assembly}

The assembly of the \textit{Rhodobacter sphaeroides} 2.4.1 data set
using the uncompressed reads outperforms all compression methods in
terms of \textsc{lap}, NG50, relocations, translations, and inversions
(Table \ref{tab:assembly_ranks}). Among the compression methods, the
256- and 32-profile encoding had the highest rank, then the 4th-order
polynomial regression, the 5th-order polynomial regression, the 2-bin
encoding, and lastly, the 3rd through 0-order polynomials.

The lossy compression methods largely preserve the contiguity found in
the assembly produced using the reads with unmodified quality
sequences. All compression methods other than 0-order polynomial
regression produce an NG50 ranging from 3.18--3.19 Mbps. Despite the
similar contiguity statistics, the different compression methods vary
noticeably in the amount of \textsc{snp}s. The order of polynomial has
an inverse relationship with the amount of \textsc{snp}s detected. The
2-bin and profile methods detected the least amount of \textsc{snp}s
compared to the reference genome, outperforming the assembly using the
original quality sequences. A more in-depth evaluation is needed to
determine whether these compression methods are missing actual
\textsc{snp}s.

It is important to highlight the result that the 4th-order polynomial
regression outperforms the 5th-order in all of the reference-based
metrics, but not the \emph{de novo} \textsc{lap} metrics. The
5th-order polynomial regression assembles approximately 2.5 kb more
sequence and uses roughly $1.2\%$ more of the total read set than the
4th-order (Supplemental Table 1), which may explain the discrepency in
\textsc{lap} values because the \textsc{lap} value is influenced by
total coverage~\citep{Ghodsi:2013hb}.

\begin{figure}[!tbp]%figure2
%\centerline{\includegraphics[width=3.65in]{mse_frag.png}}
\centerline{\includegraphics[width=3.65in]{rhodo_assembly_results.pdf}}
\caption{Rankings of different assemblies produced using proposed
  compression methods. Assemblies were constructed using
  \textsc{allpaths-lg} and the \textit{Rhodobacter sphaeroides}.
  Rankings above the median value are in cyan, those below in
  magenta}
  \label{tab:assembly_ranks}
\end{figure}

\subsection{Effects on read mapping}

Certain short read alignment tools use the quality sequence
information when finding potential alignments. Bowtie2 (version 2.2.3)
was used to evaluate the different decompressed \textsc{fastq}
files. Bowtie2 uses quality values written in the \textsc{fastq} files
when mapping reads against a reference genome. The original
uncompressed and decompressed \textsc{fastq} files were mapped with
Bowtie2 against \textit{Rhodobacter sphaeroides} reference genome. The
generated \textsc{sam} file for each compressing approach were
compared with the uncompressed \textsc{sam} file. The total, shared
and unique proportional numbers of mapped reads are calculated with
respect to the uncompressed \textsc{sam} matches as shown in table
\ref{tab:aligner}. Additionally, to monitor the effect of quality
values on mapping in general, Bowtie2 was adjusted so that the maximum
and minimum mismatch penalty were equivalent to maximum and minimum
quality scores (with parameters: --mp 6,6 and --mp 2,2 respectively).

Generally, increasing the regression model polynomial order from 0 to
5 results in more mapped reads with the most marked change from order
1 to order 2. The best compression approach is the one that has
highest proportion of reads shared between the uncompressed and
decompressed files and least number of reads that are unique in both
uncompressed and decompressed files. In fragment sample, on average,
99.03\% of reads were mapped using different compression approaches,
within which 99\% are shared with the uncompressed alignments and
0.02\% unique mapped reads in the decompressed file. The average
missed alignments are 0.99\%. Compression with profile 256 approach
was shown to be the best performing approach in the fragment
decompressed sample, as a proportional of 99.94\% reads are mapped
with respect to the uncompressed mapped reads within which 99.91\% are
common and only 0.08\% unique mapped reads in the decompressed
file. In short-run sample, the average number of mapped reads is 99.14
\% within which 98.73\% are common and 0.41\% unique mapped reads in
the decompressed file.The average missed alignments are
1.2\%. Regression with the 5th polynomial order captured the highest
mapped reads of 100\%, within which 99.66\% are common and only 0.33\%
unique mapped reads in the decompressed file.


\begin{table*}[!tbhp]
\centering
\caption[]{Quality filtering/trimming for the uncompressed and
  decompressed \textsc{fastq} files. Numbers correspond to the number
  of reads that are kept or discarded after applying compression with
  respect to the original, uncompressed values.}
\label{tab:quality_control}
\begin{tabular}{lr|cc|cc|cc|cc|cc}
 & & \multicolumn{2}{c|}{{\em MaxQual}} & \multicolumn{2}{c|}{{\em MinQual}} & \multicolumn{2}{c|}{{\em 2-bin}} & \multicolumn{2}{c|}{{\em Degree 0}} & \multicolumn{2}{c}{{\em Degree 1}} \\
 & & kept & discarded & kept & discarded & kept & discarded & kept & discarded & kept & discarded \\ 
  \cline{2-12}
& kept & 883576 &   0 &   0 & 883576 & 871035 & 12541 & 728349 & 155227 & 871510 & 12066 \\ 
{\em original} & discarded & 141858 &   0 &   0 & 141858 & 1882 & 139976 & 1475 & 140383 & 9214 & 132644 \\ 
\cline{2-12}
 & sum & 1025434 &   0 &   0 & 1025434 & 872917 & 152517 & 729824 & 295610 & 880724 & 144710 \\ 
\end{tabular}

\bigskip

\begin{tabular}{lr|cc|cc|cc|cc|cc}
 & & \multicolumn{2}{c|}{{\em Degree 3}} & \multicolumn{2}{c|}{{\em Degree 5}} & \multicolumn{2}{c|}{{\em Degree 7}} & \multicolumn{2}{c|}{{\em Profile (64)}} & \multicolumn{2}{c}{{\em Profile (128)}} \\
 & & kept & discarded & kept & discarded & kept & discarded & kept & discarded & kept & discarded \\ 
\cline{2-12}
& kept & 872006 & 11570 & 878936 & 4640 & 877802 & 5774 & 876149 & 7427 & 877513 & 6063 \\ 
{\em original}  &  discarded & 10369 & 131489 & 10004 & 131854 & 8821 & 133037 & 15795 & 126063 & 17453 & 124405 \\
\cline{2-12}
&  sum & 882375 & 143059 & 888940 & 136494 & 886623 & 138811 & 891944 & 133490 & 894966 & 130468 \\
\end{tabular}

\bigskip

\begin{tabular}{lr|cc|cc|cc|cc|cc}
&  & \multicolumn{2}{c|}{{\em Profile (256)}} & \multicolumn{2}{c|}{{\em QualComp (6)}} & \multicolumn{2}{c|}{{\em QualComp (10)}} & \multicolumn{2}{c|}{{\em QualComp (30)}} & \multicolumn{2}{c}{{\em QualComp (100)}} \\
 & &  kept & discarded & kept & discarded & kept & discarded & kept & discarded & kept & discarded \\ 
\cline{2-12}
& kept & 881215 & 2361 & 878702 & 4874 & 879682 & 3894 & 881948 & 1628 & 881939 & 1637 \\ 
{\em original}  & discarded & 15985 & 125873 & 13434 & 128424 & 12855 & 129003 & 10872 & 130986 & 7179 & 134679 \\ 
\cline{2-12}
&  sum & 897200 & 128234 & 892136 & 133298 & 892537 & 132897 & 892820 & 132614 & 889118 & 136316 \\ 
\end{tabular}
\end{table*}

\begin{table*}[!tbhp]
\centering
\caption[]{Mapping results of decompressed \textsc{fastq} files
  against \textit{Rhodobacter sphaeroides} reference genome using
  Bowtie2. Numbers corresponds to the proportion of mapped reads with
  respect to the uncompressed \textsc{fastq}. ``Shared'' denotes the
  percentage of mapped reads by both the uncompressed and decompressed
  data. ``Uncompressed only'' denotes the percentage of reads mapped
  from the uncompressed data that are not mapped after
  decompression. ``Compressed only'' denotes the percentage of reads
  mapped from the decompressed data that were not mapped before
  compression.}
\begin{tabular}{lr|cc|cc|cc|cc|cc}
 & & \multicolumn{2}{c|}{{\em MaxQual}} & \multicolumn{2}{c|}{{\em MinQual}} & \multicolumn{2}{c|}{{\em 2-bin}} & \multicolumn{2}{c|}{{\em Degree 0}} & \multicolumn{2}{c}{{\em Degree 1}} \\
& & mapped & unmapped & mapped & unmapped & mapped & unmapped & mapped & unmapped & mapped & unmapped \\ 
\cline{2-12}
& mapped & 746716 & 145897 & 892613 &   0 & 891864 & 749 & 851682 & 40931 & 883390 & 9223 \\ 
{\em original} & unmapped &   0 & 132821 & 10821 & 122000 & 186 & 132635 &  67 & 132754 &  55 & 132766 \\ 
\cline{2-12}
& sum & 746716 & 278718 & 903434 & 122000 & 892050 & 133384 & 851749 & 173685 & 883445 & 141989 \\ 
\end{tabular}

\bigskip

\begin{tabular}{lr|cc|cc|cc|cc|cc}
 & & \multicolumn{2}{c|}{{\em Degree 3}} & \multicolumn{2}{c|}{{\em Degree 5}} & \multicolumn{2}{c|}{{\em Degree 7}} & \multicolumn{2}{c|}{{\em Profile (64)}} & \multicolumn{2}{c}{{\em Profile (128)}} \\
& & mapped & unmapped & mapped & unmapped & mapped & unmapped & mapped & unmapped & mapped & unmapped \\ 
\cline{2-12}
& mapped & 889537 & 3076 & 891019 & 1594 & 891479 & 1134 & 891753 & 860 & 891952 & 661 \\ 
{\em original} & unmapped & 117 & 132704 & 155 & 132666 & 154 & 132667 & 144 & 132677 & 143 & 132678 \\ 
\cline{2-12}
& sum & 889654 & 135780 & 891174 & 134260 & 891633 & 133801 & 891897 & 133537 & 892095 & 133339 \\ 
\end{tabular}

\bigskip

\begin{tabular}{lr|cc|cc|cc|cc|cc}
&  & \multicolumn{2}{c|}{{\em Profile (256)}} & \multicolumn{2}{c|}{{\em QualComp (6)}} & \multicolumn{2}{c|}{{\em QualComp (10)}} & \multicolumn{2}{c|}{{\em QualComp (30)}} & \multicolumn{2}{c}{{\em QualComp (100)}} \\
& &  mapped & unmapped & mapped & unmapped & mapped & unmapped & mapped & unmapped & mapped & unmapped \\ 
\cline{2-12}
& mapped & 892051 & 562 & 891375 & 1238 & 891777 & 836 & 892233 & 380 & 892454 & 159 \\ 
{\em original}  & unmapped & 119 & 132702 & 304 & 132517 & 265 & 132556 & 220 & 132601 & 172 & 132649 \\ 
\cline{2-12}
& sum & 892170 & 133264 & 891679 & 133755 & 892042 & 133392 & 892453 & 132981 & 892626 & 132808 \\ 

\end{tabular}

\label{tab:aligner}
\end{table*}


\section{Discussion}

\subsection{Lossy compression acceptable for subsequent biological analyses}

The primary concern of using lossy compression methods is naturally
the extent of information loss, that we quantified by \textsc{mse} in
this study. \textsc{mse} and compressibility provide information in
the theoretical context to the methods, but they are not the end-all
of evaluation criteria. The performance of compressed datasets in
different subsequent analyses and applications are just as
important. Our benchmarks showed that some of the compression methods
with high error rates are still practical for certain kinds of
applications. Many subsequent tools proved to have enough additional
redundancy built-in to handle such loss in information. Passing the
decompressed quality sequences through quality control software shows
that most methods filter nearly as many bases as using original
quality sequences. Assemblers performing sequencing alignment use
percent similarity scores that are typically robust to standard
sequencing errors.

\subsection{Extension of 2-bin encoding}

2-bin encoding has the nice property of being simple to compute and
has good bits/base-call values. The 2-bin encoding suffers from having
a high \textsc{mse}, but fortunately, we have shown that in the case
of genome assembly, 2-bin encoding outperforms all polynomial
regressions encodings with degree less than 3. 2-bin encoding of the
fragment and short-jump libraries of \textit{Rhodobacter sphaeroides}
have \textsc{mse}s of $2.42\times$ and $10.76\times$ the 3rd-order
polynomial regression encodings, respectively. This further highlights
the importance of using additional contextual information of the
subsequent analyses when evaluating compressed quality values.

A potential extension to 2-bin encoding is to incorporate an
additional bin (\emph{okay}). The \emph{okay} value can be used where
the base qualities fall within a 2-bin range. Because the distribution
of quality values is skewed towards higher quality, we need to
experiment with different cutoffs for the \emph{okay} value and
determine if the additional storage is noticeable in subsequent
analyses.

\subsection{Potential for operations on compressed data}

Perhaps one of the greatest potential benefits of compressing quality
values is the potential to perform quality control and possibly other
operations directly on the compressed representations of the
data. This is easiest to to consider for profile-based
compression. The $k$ profiles can be evaluated for (pre-)processing
operations such as filtering and trimming, and the operations
transitively applied to the entire set of reads, thus saving
substantial computation associated with evaluating the full set of
reads.

\subsection{Future of lossy compression in bioinformatics analyses}

We have simply provided here the initial steps in analyzing the effect
of lossy compression on quality sequences using a single,
high-coverage bacterial data set. More work needs to be done using
additional biological data sets, such as human and mouse, along with
different sequencing technologies. A more direct comparison against
related lossy compression tools, such as
\textsc{SlimGene}~\citep{Kozanitis:2011kl} and
\textsc{QualComp}~\citep{Ochoa:2013rt}, needs to be
performed. Additionally, other types of sequencing data can be
analyzed apart from the Illumina data examined here. For example, the
PacBio sequencing instruments produce very long reads (with average
read lengths on the order of 10 kbp), but with the trade-off of having
a high error-rate ($\sim$15\%). Unlike the class of quality values we
have examined here, the distribution of erroneous bases is relatively
uniform~\cite{Ferrarini:2013vf}. The assembly complexity of bacterial
genomes can be greatly simplified, producing near complete genome
assemblies, by utilizing a single run of these long
reads~\citep{Koren:2013ye}. If long read sequencing technologies such
as PacBio become more widely adopted, it would be of huge benefit to
examine the potential of lossy compression algorithms on not only the
quality values, but the biological sequencing data themselves.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     please remove the " % " symbol from \centerline{\includegraphics{fig01.eps}}
%     as it may ignore the figures.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

In this paper we have examined lossy compression on sequence quality
values and their effect on subsequent analyses. Although most previous
examinations on lossy compression primarily focused on information
loss, we have shown that typically used bioinformatics software today
have additional built-in sensitivity to handle significant loss of
information in the compressed quality values.

\section*{Acknowledgement}
We thank the organizers and participants of the 2014 University of
T\"{u}bingen and University of Maryland Bioinformatics Exchange for
Students and Teachers (BEST) Summer School.

\bibliographystyle{natbib}
\bibliographystyle{achemnat}
\bibliographystyle{plainnat}
\bibliographystyle{abbrv}
\bibliographystyle{bioinformatics}
%
\bibliographystyle{plain}
%

\bibliography{compression}

\end{document}
